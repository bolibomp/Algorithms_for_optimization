{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.1**  Compute the gradient of $\\mathbf{x}^T\\mathbf{A}\\mathbf{x} + \\mathbf{b}^T\\mathbf{x}$ when $\\mathbf{A}$ is a symmetric matrix.\n",
    "\n",
    "**Answer:**\n",
    "We have\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\nabla_{\\mathbf{x}}\\left(\\mathbf{x}^T\\mathbf{A}\\mathbf{x} + \\mathbf{b}^T\\mathbf{x}\\right)\\\\\n",
    "&= \\nabla_{\\mathbf{x}}\\left(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) + \\nabla_{\\mathbf{x}}\\left(\\mathbf{b}^T\\mathbf{x}\\right)\\\\\n",
    "&= \\nabla_{\\mathbf{x}}\\left(\\mathbf{x}^T\\mathbf{A}\\mathbf{x}\\right) + \\mathbf{b}\\\\\n",
    "&= \\mathbf{x}^T\\nabla_{\\mathbf{x}}\\left(\\mathbf{A}\\mathbf{x}\\right) + \\nabla_{\\mathbf{x}}\\left(\\mathbf{x}^T\\right)\\mathbf{A}\\mathbf{x} + \\mathbf{b}\\\\\n",
    "&= \\mathbf{x}^T\\mathbf{A} + \\mathbf{A}^T\\mathbf{x} + \\mathbf{b}\\\\\n",
    "&= \\left(\\mathbf{A} + \\mathbf{A}^T\\right)\\mathbf{x} + \\mathbf{b}\\\\\n",
    "&= 2\\mathbf{A}\\mathbf{x} + \\mathbf{b}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we have used that $A=A^T$ since $\\mathbf{A}$ is symmetric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.2.**  Apply gradient descent with a unit step size to $f(x)=x^4$ from a starting point of your choice. Compute two iterations.\n",
    "\n",
    "**Answer:**\n",
    "Lets start with $x_0=1$ with $\\alpha=1$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(x_0) &= 1^4\\\\\n",
    "&= 1\\\\\n",
    "f'(x) &= 4x^3\\\\\n",
    "x_1 &= x_0 - \\alpha f'(x_0)\\\\\n",
    "&= 1 - 1\\cdot 4\\cdot 1^3\\\\\n",
    "&= -3\\\\\n",
    "f(x_1) &= (-3)^4\\\\\n",
    "&= 81\\\\\n",
    "x_2 &= x_1 - \\alpha f'(x_1)\\\\\n",
    "&= -3 - 1\\cdot 4\\cdot (-3)^3\\\\\n",
    "&= -3 - 1\\cdot 4\\cdot (-27)\\\\\n",
    "&= 105\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.3.** Apply one step of gradient descent to $f(x)=e^x+e^{-x}$ from $x^{(1)}=10$ with both a unit step size and with exact line search.\n",
    "\n",
    "**Answer:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "f'(x) &= e^x - e^{-x}\\\\\n",
    "x^{(2)} &= x^{(1)} - f'(x^{(1)})\\\\\n",
    "&= 10 - \\left(e^{10} - e^{-10}\\right)\\\\\n",
    "f(x^{(2)}) &= e^{x^{(2)}} + e^{-x^{(2)}}\\\\\n",
    "&= e^{10 - \\left(e^{10} - e^{-10}\\right)} + e^{-10 + \\left(e^{10} - e^{-10}\\right)}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "For exact line search we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left(- e^{10} + e^{-10}\\right) e^{- \\alpha \\left(- \\frac{1}{e^{10}} + e^{10}\\right) + 10} + \\left(- \\frac{1}{e^{10}} + e^{10}\\right) e^{\\alpha \\left(- \\frac{1}{e^{10}} + e^{10}\\right) - 10}$"
      ],
      "text/plain": [
       "(-exp(10) + exp(-10))*exp(-\\alpha*(-exp(-10) + exp(10)) + 10) + (-exp(-10) + exp(10))*exp(\\alpha*(-exp(-10) + exp(10)) - 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, a = sp.symbols(r'x \\alpha')\n",
    "f = sp.exp(x) + sp.exp(-x)\n",
    "df_dx = sp.diff(f, x)\n",
    "\n",
    "sub_f = f.subs(x, 10 - a * df_dx.subs(x, 10))\n",
    "\n",
    "sp.diff(sub_f, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{5}{\\sinh{\\left(10 \\right)}}$"
      ],
      "text/plain": [
       "5/sinh(10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_a = sp.solve(sp.diff(sub_f, a), a)[0]\n",
    "optimal_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "x^{(2)} &= x^{(1)} - \\alpha f'(x^{(1)})\\\\\n",
    "&= 10 - \\frac{5}{\\sinh(10)} \\left(e^{10} - e^{-10}\\right)\\\\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.4.** The conjugate gradient method can also be used to find a search direction $\\mathbf{d}$ when a local quadratic model of a function is available at the current point. With $\\mathbf{d}$ as search direction, let the model be\n",
    "\n",
    "$$\n",
    "\\mathit{q}(\\mathbf{d}) = \\mathbf{d}^T\\mathbf{H}\\mathbf{d} + \\mathbf{b}^T\\mathbf{d} + \\mathbf{c}\n",
    "$$\n",
    "\n",
    "for a symmetric matrix $\\mathbf{H}$. What is the Hessian in this case? What is the gradient of $\\mathit{q}$ when $\\mathbf{d}=0$? What can go wrong if the conjugate gradient method is applied to the quadratic model to get the search direction $\\mathbf{d}$?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hessian is $2\\mathbf{H}$. The gradient is\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{d}}\\mathit{q}(\\mathbf{d}) = 2\\mathbf{d}^T\\mathbf{H} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{d}=0$ then $\\nabla_{\\mathbf{d}}\\mathit{q}(\\mathbf{d}) = \\mathbf{b}$. If the conjugate gradient method is applied to the quadratic model to get the search direction $\\mathbf{d}$ then the search direction will be $\\mathbf{d}=-\\mathbf{b}$.\n",
    "\n",
    "If $\\mathbf{H}$ is not positive definite then it's not locally convex and the method might not converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.5.**  How is Nesterov momentum an improvement over momentum?\n",
    "\n",
    "**Answer:**\n",
    "Regular momentum uses the cumulative sum of the gradients to update the parameters and this can lead to overshooting. Nesterov momentum uses the gradient at the next step to update the parameters and this can lead to faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.6.**  In what way is the conjugate gradient method an improvement over steepest descent?\n",
    "\n",
    "**Answer:**\n",
    "The steepes descent method uses the gradient to update the parameters and this can lead to zig-zagging as the direction are orthogonal to each other. The conjugate gradient method uses the gradient and the previous direction and the mutual conjugate between the direction in an approximate quadratic model to update the parameters and this can lead to faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.7.**  In conjugate gradient descent, what is the normalized descent direction at the first iteration for the function $f(x,y)=x^2 + xy + y^2+5$ when initialized at $(x,y)=(1,1)$? What is the resulting point after two steps of conjugate gradient method?\n",
    "\n",
    "**Answer:**\n",
    "Since $f(x)=x^2 + xy + y^2+5$ is a quadratic function we can calculate the gradient, the Hessian, the step size and the descent direction exactly.\n",
    "Let's rewrite the function in matrix form\n",
    "\n",
    "$$\n",
    "f(x) = \\mathbf{x}^T\\mathbf{A}\\mathbf{x} + \\mathbf{b}^T\\mathbf{x} + c\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\begin{bmatrix}x\\\\y\\end{bmatrix}, \\mathbf{A} = \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}, \\mathbf{b} = \\begin{bmatrix}0\\\\0\\end{bmatrix}, c = 5\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}}f(\\mathbf{x}) = \\mathbf{A}\\mathbf{x} + \\mathbf{b}\n",
    "$$\n",
    "\n",
    "The Hessian is\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}}^2f(\\mathbf{x}) = \\left(\\mathbf{A} + \\mathbf{A}^T\\right) = 2\\mathbf{A}\n",
    "$$\n",
    "\n",
    "where we have used that $\\mathbf{A}$ is symmetric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first direction is the negative normalized gradient at the starting point\n",
    "\n",
    "$$\n",
    "\\mathbf{d}_1 = -\\frac{\\nabla_{\\mathbf{x}}f(\\mathbf{x}_0)}{\\left\\lVert\\nabla_{\\mathbf{x}}f(\\mathbf{x}_0)\\right\\rVert} = -\\frac{\\mathbf{A}\\mathbf{x}_0 + \\mathbf{b}}{\\left\\lVert\\mathbf{A}\\mathbf{x}_0 + \\mathbf{b}\\right\\rVert} = -\\frac{\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}0\\\\0\\end{bmatrix}}{\\left\\lVert\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}0\\\\0\\end{bmatrix}\\right\\rVert} = -\\frac{\\begin{bmatrix}3\\\\3\\end{bmatrix}}{\\left\\lVert\\begin{bmatrix}3\\\\3\\end{bmatrix}\\right\\rVert} = -\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The optimal step size is\n",
    "\n",
    "$$\n",
    "\\alpha_1 = -\\frac{\\mathbf{d}_1^T\\left(\\mathbf{A}\\mathbf{x}_0 + \\mathbf{b}\\right)}{\\mathbf{d}_1^T\\mathbf{A}\\mathbf{d}_1} = -\\frac{\\left(-\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right)^T\\left(\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}0\\\\0\\end{bmatrix}\\right)}{\\left(-\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right)^T\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}\\left(-\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1\\\\1\\end{bmatrix}\\right)} = -\\frac{\\frac{1}{\\sqrt{2}}\\begin{bmatrix}3\\\\3\\end{bmatrix}}{\\frac{1}{2}\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}\\begin{bmatrix}1\\\\1\\end{bmatrix}} = -\\frac{\\frac{1}{\\sqrt{2}}\\begin{bmatrix}3\\\\3\\end{bmatrix}}{\\frac{1}{2}\\begin{bmatrix}3\\\\3\\end{bmatrix}} = -\\frac{2}{\\sqrt{2}} = -\\sqrt{2}\n",
    "$$\n",
    "\n",
    "Again since $f(x)=x^2 + xy + y^2+5$ is a quadratic function we know that this method converges in n steps where n is the number of dimensions. Since we have two dimensions we know that the method will converge in two steps at the optimal point of $(x,y)=(0,0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.8.**  We have a polynomial function $f$ such that $f(\\mathbf{x})>2$ for all $\\mathbf{x}$ in three-dimensional Euclidean space. Suppose we are using steepest descent with step lenghts optimized at each step, and we want to find a local minimum of $f$. If our unnormalized decent direction is $[1,2,3]^T$ at step $k$, is it possible four our unnormalized descent direction at step $k+1$ to be $[0,0,-3]^T$? Why or why not?\n",
    "\n",
    "**Answer:**\n",
    "No, it's not possible. Given that we have optimal step lengths at each step we know that the descent direction is orthogonal to the gradient at each step. Since the gradient is orthogonal to the descent direction at each step we know that the descent direction at step $k+1$ must be orthogonal to the descent direction at step $k$. Since \n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix}\\cdot\\begin{bmatrix}0\\\\0\\\\-3\\end{bmatrix} \\neq 0\n",
    "$$\n",
    "\n",
    "we know that the descent direction at step $k+1$ can't be $[0,0,-3]^T$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
